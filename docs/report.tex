
\documentclass[conference]{IEEEtran}
\usepackage{blindtext, graphicx}



% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi



% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Quote Attribution Using Recurrent Neural Networks}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Sayan Das}
\IEEEauthorblockA{Computer Science and Engineering\\
SRM University, Kattankulathur\\
Chennai, India 603203\\
Email: sayan\_das@srmuniv.edu.in}}


% make the title area
\maketitle

% Abstract Block
\begin{abstract}
This paper proposes Defferent Neural Network Models for attributing dialogoues on Literary Texts. Quote Attribution, is the problem of tagging the dialogues with the speaker, is import for task like deanonymizing blogs, article or documents published illegally over world wide web, text mining models and as a media monitoring system. We used Recurrent Neural Models with defferent cells to establish the advantages and weak points among them.
\end{abstract}
\begin{IEEEkeywords}
Natural Language Processing, Neural Networks, Machine Learning
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle


\section{Introduction}
Conversation is the fundamental means for human interaction. Having back and forth interaction helps to understand the way one thinks, thus having a mutual understanding. It’s the beauty of brain how after a prolonged observation of same person conversing it starts to infer patterns. These patterns can either be the repetition of words or unique grammatical structure.
Here  in this project I will try to replicate the same model using Natural Language Processing and Neural Networks. For this given a quote the model will try to interpret the speaker based purely on the language based approach, i.e. no visual and auditory data are provided. Model will try to interpret context by using clues like “he said …” and “she told …” to understand the gender of the speaker. This can be achieved by using Recurrent Neural Network (RNN) which stores the context upto a extent in the form of weights. Evaluation can be done by applying the predictive model to TV series episode scripts. 



\section{Dataset}
For the training and evaluation purpose I have used scripts of TV series particularly Seinfield. Script files have been collected by scraping \emph{https://imsdb.com}. Several text preprocessing techniques are applied to clean the raw data.

\section{Text Preprocessing and Word Vector} 
Entire datasets have been cleaned by removing all the punctuations and bracket content, then converted to lower case and tokenised using NLTK tokeniser. We have used Stanford GloVe vector trained on \emph{6 billion Gigaword5 + Wikipedia2014 Corpus} as the input layer for all the models. Given input word $w_i$ from the dialogue, $x_i$ be the one-hot row vector of the corresponding word from vocubulary(V) and $L \in  R^{|V| \times d}$  be the embedding matrix, where $d$ is the dimension of the word vector. Then,

\begin{center}
	$w_i = x_i \times L$
\end{center}

\section{Quote Vector}
Given a quote $q$ consists of $\{w_1, w_2, w_3 ... w_n\}$ word vectors and the number of words ($n$) in each quote varies. Each word vector $w_i$ can be represented by $\{f_1^i, f_2^i, f_3^i ... f_d^i\}$. here in this case the dimension of the quote is of $n \times d$, to reduce the dimension we have taken the summary of each quote and thus reducing the dimension to $d$.

\begin{center}
	$$\overrightarrow{\rm q} = \frac{1}{n} \sum_{i=1}^{n} f_k^i , \forall k \in \{1,2,3 ... d\} $$
\end{center}

\section{Models}
\subsection{n-Way Multi Layered Perceptron}
This model consist of 1 hiddenl layer with softmax output layer, that takes quote vector ($\overrightarrow{\rm q}$) as input. Input layer consist of \emph{d} nodes and output layer consist on \emph{n} (\emph{n} is number of speakers) nodes. It requires predetermined list of speaker's list.

\begin{center}
	$f(\overrightarrow{\rm q}) = W.\overrightarrow{\rm q} + b$\\
	$h = sigmoid(f(\overrightarrow{\rm q}))$\\
	$\hat{y} = softmax(h)$
\end{center}

Then the speaker can be predicted by $argmax_i(\hat{y})$, where in \emph{i} is the index of speaker in speaker list.

\appendices
\section{Proof of the First Zonklar Equation}
\blindtext

% use section* for acknowledgement
\section*{Acknowledgment}


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{picture}}]{John Doe}
\blindtext
\end{IEEEbiography}

\end{document}


